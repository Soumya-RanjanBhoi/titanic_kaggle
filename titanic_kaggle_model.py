# -*- coding: utf-8 -*-
"""Titanic_kaggle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18FXBmjqEVdl8yFOFZ67M1OoT5di74wC1
"""

import pandas as pd
import seaborn as sns
import numpy as np

import zipfile

with zipfile.ZipFile("/content/titanic.zip") as z:
    with z.open('train.csv') as f:
        df_train = pd.read_csv(f)

with zipfile.ZipFile("/content/titanic.zip") as z:
    with z.open('test.csv') as f:
        df_test = pd.read_csv(f)

df_train

df_train.isnull().sum()

df_train.info()

sns.boxplot(df_train)

col = ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']

for coln in col:
  Q1=df_train[coln].quantile(.25)
  Q3=df_train[coln].quantile(.75)

  IQR=Q3-Q1

  lower_bound= Q1 - IQR * 1.5
  upper_bound= Q3 + IQR *1.5

  df_train[coln] = np.where(
      df_train[coln]>upper_bound,
      upper_bound,
      np.where(
          df_train[coln]<lower_bound,
          lower_bound,
          df_train[coln]
      )
  )

df_train.drop(["Ticket","Name","Cabin"],axis=1,inplace=True)

df_train.sample(5)

X=df_train.drop(["Survived"],axis=1)
y=df_train["Survived"]

X.shape

from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier

coln_transformer=ColumnTransformer([
    ("tsf1",OneHotEncoder(sparse_output=False),["Sex","Embarked"]),
    ("tsf2",SimpleImputer(strategy="mean"),["Age"]),
    ("tsf3",StandardScaler(),["PassengerId","Fare"])
],remainder="passthrough")

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)

x_train=coln_transformer.fit_transform(x_train)
x_test=coln_transformer.transform(x_test)

from sklearn.linear_model import LogisticRegression

lr=LogisticRegression(
    penalty="l2",
    max_iter=1000,
    random_state=42

)

lr.fit(x_train,y_train)

y_pred=lr.predict(x_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test,y_pred)*100

model_par=[("lr1",LogisticRegression(max_iter=1000)),("dtc",DecisionTreeClassifier()),("rf",RandomForestClassifier())]

vc=VotingClassifier(
    estimators=model_par,
    voting="soft"
)

vc.fit(x_train,y_train)

y_pred1=vc.predict(x_test)

from sklearn.model_selection import cross_val_score

print(accuracy_score(y_test,y_pred1))
X_transformed = coln_transformer.fit_transform(X)
print(cross_val_score(vc,X_transformed,y,cv=10).mean())

!pip install optuna

import optuna

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

def objective(trial):
    classifier_name = trial.suggest_categorical(
        "classifier",
        ["GradientBoostingClassifier", "DecisionTreeClassifier", "RandomForestClassifier", "XGBClassifier"]
    )

    if classifier_name == "GradientBoostingClassifier":
        learning_rate = trial.suggest_float("gb_learning_rate", 0.01, 0.3)
        n_estimators = trial.suggest_int("gb_n_estimators", 100, 500)
        max_depth = trial.suggest_int("gb_max_depth", 3, 10)
        subsample = trial.suggest_float("gb_subsample", 0.5, 1.0)

        model = GradientBoostingClassifier(
            learning_rate=learning_rate,
            n_estimators=n_estimators,
            max_depth=max_depth,
            subsample=subsample
        )

    elif classifier_name == "DecisionTreeClassifier":
        criterion = trial.suggest_categorical("dt_criterion", ["gini", "entropy", "log_loss"])
        max_depth = trial.suggest_int("dt_max_depth", 10, 20)
        max_features = trial.suggest_categorical("dt_max_features", ["sqrt", "log2", None])
        min_samples_split = trial.suggest_int("dt_min_samples_split", 2, 10)

        model = DecisionTreeClassifier(
            criterion=criterion,
            max_depth=max_depth,
            max_features=max_features,
            min_samples_split=min_samples_split
        )

    elif classifier_name == "RandomForestClassifier":
        n_estimators = trial.suggest_int("rf_n_estimators", 100, 500)
        max_depth = trial.suggest_int("rf_max_depth", 10, 30)
        criterion = trial.suggest_categorical("rf_criterion", ["gini", "entropy", "log_loss"])
        bootstrap = trial.suggest_categorical("rf_bootstrap", [True, False])
        max_features = trial.suggest_float("rf_max_features", 0.3, 1.0)

        max_samples = trial.suggest_float("rf_max_samples", 0.3, 1.0) if bootstrap else None

        model = RandomForestClassifier(
            n_estimators=n_estimators,
            criterion=criterion,
            max_depth=max_depth,
            max_features=max_features,
            bootstrap=bootstrap,
            max_samples=max_samples
        )

    elif classifier_name == "XGBClassifier":
        learning_rate = trial.suggest_float("xgb_learning_rate", 0.01, 0.3)
        n_estimators = trial.suggest_int("xgb_n_estimators", 100, 500)
        max_depth = trial.suggest_int("xgb_max_depth", 3, 10)
        subsample = trial.suggest_float("xgb_subsample", 0.5, 1.0)
        colsample_bytree = trial.suggest_float("xgb_colsample_bytree", 0.5, 1.0)
        gamma = trial.suggest_float("xgb_gamma", 0, 5)
        reg_alpha = trial.suggest_float("xgb_reg_alpha", 0, 5)
        reg_lambda = trial.suggest_float("xgb_reg_lambda", 0, 5)

        model = XGBClassifier(
            learning_rate=learning_rate,
            n_estimators=n_estimators,
            max_depth=max_depth,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            gamma=gamma,
            reg_alpha=reg_alpha,
            reg_lambda=reg_lambda,
            use_label_encoder=False,
            eval_metric="logloss",
            verbosity=0
        )

    score = cross_val_score(model, x_train, y_train, cv=5, scoring="accuracy").mean()
    return score

study=optuna.create_study(direction="maximize",sampler=optuna.samplers.TPESampler())
study.optimize(objective,n_trials=80)

study.best_params

study.best_trial.value

model_new=XGBClassifier(
    xgb_learning_rate= 0.11874895219087726,
    xgb_n_estimators= 365,
    xgb_max_depth= 10,
    xgb_subsample=0.8967426183507576,
    xgb_colsample_bytree= 0.9058082699029233,
    xgb_gamma= 1.3524552933557932,
    xgb_reg_alpha=0.11315939051174163,
    xgb_reg_lambda= 2.1842287427738243
)

model_new.fit(x_train,y_train)

pred=model_new.predict(x_test)
accuracy_score(y_test,y_pred)

model_=RandomForestClassifier(
      n_estimators= 500,
      max_depth= 16,
      criterion='entropy',
      bootstrap= True,
      max_samples=0.4549896515836023,
      max_features= 0.5118779632183504
    )

model_.fit(x_train,y_train)

y_pred2=model_.predict(x_test)

accuracy_score(y_test,y_pred2)

cross_val_score(model_new,x_train,y_train,cv=10).mean()

df_test.drop(["Name","Ticket","Cabin"],axis=1,inplace=True)

df_test

Passenger_id=df_test["PassengerId"].sort_values()

Passenger_id

df_test.isnull().sum()

sns.boxplot(df_test)

col_new=['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']

for coln in col_new:
  Q1=df_test[coln].quantile(.25)
  Q3=df_test[coln].quantile(.75)

  IQR=Q3-Q1

  lower_bound= Q1 - IQR * 1.5
  upper_bound= Q3 + IQR *1.5

  df_test[coln] = np.where(
      df_test[coln]>upper_bound,
      upper_bound,
      np.where(
          df_test[coln]<lower_bound,
          lower_bound,
          df_test[coln]
      )
  )

imputer = SimpleImputer(strategy="most_frequent")
df_test = pd.DataFrame(df_test)

for column in ['Age', 'Embarked', 'Fare']:
  if column in df_test.columns:
     df_test[column] = imputer.fit_transform(df_test[[column]]).ravel()

df_test=coln_transformer.transform(df_test)

prediction=pd.DataFrame({"PassengerId":Passenger_id,"Survived":vc.predict(df_test)})

prediction.to_csv("Submission2.csv",index=False)

